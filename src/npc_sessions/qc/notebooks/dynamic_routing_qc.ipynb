{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import npc_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = npc_sessions.Session(\n",
    "    R'\\\\allen\\programs\\mindscope\\workgroups\\dynamicrouting\\PilotEphys\\Task 2 pilot\\DRpilot_662892_20230824'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tuple((device.name, device.sampling_rate, device.start_time) for device in npc_sessions.get_ephys_timing_on_sync(session.sync_path, session.ephys_recording_dirs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sync_data.meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = session.sync_data.plot_diode_measured_sync_square_flips()\n",
    "stim_display_times = npc_sessions.get_stim_frame_times(*session.stim_paths, sync=session.sync_data)\n",
    "names = tuple(k for k, v in stim_display_times.items() if v is not None)\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.set_title(names[idx].stem.split('_')[0])\n",
    "fig.set_size_inches(20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = session.sync_data.plot_stim_onsets()\n",
    "names = tuple(k for k, v in stim_display_times.items() if v is not None)\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.set_title(names[idx].stem.split('_')[0])\n",
    "fig.set_size_inches(20, 5 * len(axes))\n",
    "fig.subplots_adjust(hspace=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "\n",
    "for info in session.video_info_data.values():\n",
    "    print(info['CameraLabel']), display(IPython.display.JSON(\n",
    "        {\n",
    "            k:v\n",
    "            for k,v in info.items() \n",
    "            if k in ('FPS', 'Duration', 'FramesRecorded', 'FramesLostCount')\n",
    "        } | {'Lost': f\"{100 * info['FramesLostCount'] / info['FramesRecorded']:.5f}%\"}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cv2\n",
    "import numpy as np\n",
    "import datetime\n",
    "import io\n",
    "\n",
    "import upath\n",
    "\n",
    "def camera_frame_grabs_simple(\n",
    "    paths: Iterable[upath.UPath],\n",
    "    num_frames_to_grab: int = 5,\n",
    ") -> None:\n",
    "    \"\"\"Just plots evenly spaced frames, no concept of epochs.\n",
    "    \n",
    "    video frames across cameras aren't synced .\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=[20, 3 * len(paths)], constrained_layout=True, facecolor='0.5')\n",
    "    gs = gridspec.GridSpec(len(paths), num_frames_to_grab, figure=fig)\n",
    "    gs.update(wspace=0.0, hspace=0.0)\n",
    "    for idx, video_path in enumerate(paths):\n",
    "        # get frames to plot\n",
    "        v = cv2.VideoCapture(video_path.as_posix()) # TODO open with upath from cloud\n",
    "        \n",
    "        frame_delta = np.ceil(v.get(7) / num_frames_to_grab + 1)\n",
    "        frames_of_interest = np.arange(v.get(5), v.get(7), frame_delta)\n",
    "\n",
    "        for i, f in enumerate(frames_of_interest):\n",
    "            v.set(cv2.CAP_PROP_POS_FRAMES, int(f))\n",
    "            ret, frame = v.read()\n",
    "            ax = fig.add_subplot(gs[idx, i])\n",
    "            ax.imshow(frame)\n",
    "            # ax.axis('off')\n",
    "            ax.tick_params(\n",
    "                top=False,\n",
    "                bottom=False,\n",
    "                left=False,\n",
    "                right=False,\n",
    "                labelleft=False,\n",
    "                labelbottom=False,\n",
    "            )\n",
    "            ax.set_title(datetime.timedelta(seconds=f/v.get(5)), fontsize=10)\n",
    "    return fig\n",
    "\n",
    "fig = camera_frame_grabs_simple(session.video_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify frame times can be found on sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npc_sessions.get_video_frame_times(session.sync_data, *session.video_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
